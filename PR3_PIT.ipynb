{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qncY3FktdgMI"
      },
      "source": [
        "# PIT - Práctica 3: Detección de Actividad de Voz (VAD)\n",
        "\n",
        "**Alicia Lozano Díez**\n",
        "\n",
        "10 de marzo de 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6yNO58J-7my"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "El objetivo de esta práctica es proporcionar una introducción al procesamiento de señales temporales de voz, y desarrollar de un detector de actividad de voz basado en redes neuronales recurrentes, en particular, LSTM.\n",
        "\n",
        "### Materiales\n",
        "\n",
        "- Guión (.ipynb) de la práctica - Moodle\n",
        "- Ejemplos de datos y etiquetas - Moodle\n",
        "- Listas de entrenamiento y validación - Moodle\n",
        "- Scripts de descarga de datos - Moodle\n",
        "- Datos y etiquetas de entrenamiento * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EeHT_NXP56FLkKffjyOhfa8BqAy3EmIrMkBZ0wnyDAti1g?download=1)\n",
        "- Datos y etiquetas de validación * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/ESc5XzkpZ3ZBnGFQ6HWdn_UB38NHMOLTLtTcEE_b81Cylw?download=1)\n",
        "\n",
        "\n",
        "**CUIDADO: Los datos proporcionados son de uso exclusivo para esta práctica. No tiene permiso para copiar, distribuir o utilizar el corpus para ningún otro propósito.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1BzhOOn-6H4"
      },
      "source": [
        "# 1. Introducción al procesamiento de señales temporales de voz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Descarga de ficheros de ejemplo\n",
        "\n",
        "Primero vamos a descargar el audio de ejemplo de Moodle (**audio_sample.wav**) y ejecutar las siguientes  líneas de código, que nos permitirán subir el archivo a Google Colab desde el disco local:"
      ],
      "metadata": {
        "id": "8tgJsLk6UQQG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edF0oDBFruUG"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiWqca8brveq"
      },
      "source": [
        "Una vez cargado el fichero de audio, podemos escucharlo de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UH-jL-DBPpU"
      },
      "source": [
        "import IPython\n",
        "\n",
        "wav_file_name = \"audio_sample.wav\"\n",
        "print(wav_file_name)\n",
        "IPython.display.Audio(wav_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAujomkYr41Q"
      },
      "source": [
        "## 1.2. Lectura y representación de audio en Python\n",
        "\n",
        "A continuación vamos a definir ciertas funciones para poder hacer manejo de  ficheros de audio en Python.\n",
        "\n",
        "Comenzamos definiendo una función **read_recording** que leerá un fichero de audio WAV, normalizará la amplitud y devolverá el vector de muestras _signal_ y su frecuencia de muestreo _fs_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvbxNBIGBi1O"
      },
      "source": [
        "import scipy.io.wavfile\n",
        "\n",
        "def read_recording(wav_file_name):\n",
        "  fs, signal = scipy.io.wavfile.read(wav_file_name)\n",
        "  signal = signal/max(abs(signal)) # normalizes amplitude\n",
        "\n",
        "  return fs, signal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QESNJAasdB5"
      },
      "source": [
        "Si ejecutamos la función anterior para el fichero de ejemplo, podemos ver la forma en la que se carga dicho fichero de audio en Python. Así, podemos obtener la frecuencia de muestreo y la longitud del fichero en número de muestras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISH_GeSReo8i"
      },
      "source": [
        "fs, signal = read_recording(wav_file_name)\n",
        "print(\"Signal variable shape: \" + str(signal.shape))\n",
        "print(\"Sample rate: \" + str(fs))\n",
        "print(\"File length: \" + str(len(signal)) + \" samples\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njGccLOJvoWe"
      },
      "source": [
        "Para obtener la duración en segundos de la señal, tendríamos que usar la longitud en muestras y la frecuencia de muestreo:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duración en segundos\n",
        "print(\"File length: \" + str(len(signal)/fs) + \" seconds\")"
      ],
      "metadata": {
        "id": "it26O3uZ4hRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt1f-HXntuLS"
      },
      "source": [
        "También podemos representar la señal y ver su forma de onda. Para ello, definimos la función **plot_signal** como sigue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOzyL0JXCG65"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_signal(signal, fs, ylabel=\"\", title=\"\"):\n",
        "  dur = len(signal)/fs\n",
        "  step = 1./fs\n",
        "  t_axis = np.arange(0., dur, step)\n",
        "\n",
        "  plt.plot(t_axis, signal)\n",
        "  plt.xlim([0, dur])\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.xlabel('Time (seconds)')\n",
        "  plt.title(title)\n",
        "  plt.grid(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAW3wzOxuAB-"
      },
      "source": [
        "Y utilizando la función anterior, obtenemos su representación (amplitud frente al tiempo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpjB716Yma3O"
      },
      "source": [
        "plot_signal(signal, fs, \"Amplitude\", wav_file_name)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzKKzBiuTWO"
      },
      "source": [
        "## 1.3. Representación de etiquetas de actividad de voz\n",
        "\n",
        "En esta práctica, vamos a desarrollar un detector de actividad de voz, que determinará qué segmentos de la señal de voz son realmente voz y cuáles silencio.\n",
        "\n",
        "Por ello, vamos a ver dos ejemplos de etiquetas _ground truth_, que corresponden al fichero de audio de ejemplo.\n",
        "\n",
        "Primero, descargamos de Moodle las etiquetas de voz/silencio que están en los ficheros **audio_sample_labels_1.voz** y **audio_sample_labels_2.voz** y las cargamos en Google Colab como en el caso anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM5RhBFwCx3-"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gohRGQxPFq"
      },
      "source": [
        "Estas etiquetas están guardadas en ficheros de texto y podemos cargarlas en Python de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4g5HQm6KNQL"
      },
      "source": [
        "labels_file_name = 'audio_sample_labels_1.voz'\n",
        "voice_labels = np.loadtxt(labels_file_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOarG7qcx7BV"
      },
      "source": [
        "Con el siguiente código, podemos representar la señal de voz así como sus etiquetas en la misma figura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "telfETsXx5GK"
      },
      "source": [
        "plot_signal(signal, fs)\n",
        "# voice_labels*2-1 para transformar las etiquetas de 0 y 1 a -1 y 1\n",
        "plot_signal(voice_labels*2-1, fs, \"Amplitude\", wav_file_name)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eW_pmANycy7"
      },
      "source": [
        "Las etiquetas de voz/silencio provienen de distintos detectores de actividad de voz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl3hGkNIxmuT"
      },
      "source": [
        "**PREGUNTA 1: Represente la señal de voz junto con las etiquetas para ambos casos e incluya las figuras en el informe de la práctica. ¿Qué diferencias observas? ¿A qué se puede deber? ¿Qué cantidad de voz/silencio hay en cada etiquetado?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OkNuu7sD5pMt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHj1_1fHzAed"
      },
      "source": [
        "## 1.4. Extracción de características\n",
        "\n",
        "En la mayoría de sistemas de reconocimiento de patrones, un primer paso es la extracción de características. Esto consiste, a grandes rasgos, en obtener una representación de los datos de entrada, que serán utilizados para un posterior modelado.\n",
        "\n",
        "En nuestro caso, vamos pasar de la señal en crudo _\"raw\"_ dada por las muestras (_signal_), a una secuencia de vectores de características que extraigan información a corto plazo de la misma y la representen. Esta sería la entrada a nuestro sistema de detección de voz basado en redes neuronales.\n",
        "\n",
        "Para ver algunos ejemplos, vamos a utilizar la librería _librosa_ (https://librosa.org/doc/latest/index.html).\n",
        "\n",
        "Dentro de esta librería, tenemos funciones para extraer distintos tipos de características de la señal de voz, como por ejemplo el espectrograma en escala Mel (_melspectrogram_).\n",
        "\n",
        "Estas características a corto plazo, se extraen en ventanas de unos pocos milisegundos con o sin solapamiento.\n",
        "\n",
        "Un ejemplo sería el siguiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a_7VvYYMFnQ"
      },
      "source": [
        "import librosa\n",
        "\n",
        "mel_spec = librosa.feature.melspectrogram(y=signal,sr=fs,n_mels=23,win_length=320,hop_length=160)\n",
        "\n",
        "print(mel_spec.shape)\n",
        "print(signal.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQPnf6CB1Dqh"
      },
      "source": [
        "**PREGUNTA 2: ¿Qué se obtiene de la función anterior? ¿Qué significan los valores de los parámetros _win_length_ y _hop_length_? ¿Qué significan las dimensiones de _mel_spec_?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xKjU4-oL6Fdq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwMMOuI1icD"
      },
      "source": [
        "De esta manera, podríamos obtener una parametrización de las señales para ser utilizadas como entrada a nuestra red neuronal.\n",
        "\n",
        "Para los siguientes apartados, se proporcionan los vectores de características MFCC para una serie de audios que se utilizarán como conjunto de entrenamiento del modelo de VAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64IMtv3LPTIX"
      },
      "source": [
        "#2. Detector de actividad de voz (Voice Activity Detector, VAD)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.1. Descarga de los datos de entrenamiento\n",
        "\n",
        "Primero vamos a descargar la lista de identificadores de los datos de entrenamiento de la práctica.\n",
        "\n",
        "Para ello, necesitaremos descargar de Moodle el fichero **training_VAD.lst**, y ejecutar las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
      ],
      "metadata": {
        "id": "FWgXfSysUTqQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB8wcxZxVS9g"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTthDBqzPxen"
      },
      "source": [
        "A continuación cargamos los identificadores contenidos en el fichero en una lista en Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOHSTbDiVB1y"
      },
      "source": [
        "file_train_list = 'training_VAD.lst' # mat files containing data + labels\n",
        "f = open(file_train_list, 'r')\n",
        "train_list = f.read().splitlines()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfBy_23fP-Cr"
      },
      "source": [
        "Podemos ver algunos de ellos (los primeros 10 identificatores) de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLIzBjxNQB9I"
      },
      "source": [
        "print(train_list[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8Bexbe3QSpW"
      },
      "source": [
        "Ahora, descargaremos de Moodle el fichero **data_download_onedrive_training_VAD.sh**, y ejecutaremos las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHJX10CN3KhK"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gonGnfmiQhmR"
      },
      "source": [
        "Para descargar el conjunto de datos desde One drive, ejecutamos el script cargado anteriormente de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2NmZBo-34xk"
      },
      "source": [
        "!chmod 755 data_download_onedrive_training_VAD.sh\n",
        "!./data_download_onedrive_training_VAD.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkshERjWQx_-"
      },
      "source": [
        "Este script descargará los datos de One Drive y los cargará en Google Colab, descomprimiéndolos en la carpeta **data/training_VAD**.\n",
        "\n",
        "Podemos comprobar que los ficheros **.mat** se encuentran en el directorio esperado:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Cr06cf4pe0"
      },
      "source": [
        "!ls data/training_VAD/ | head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTwj-3NjSpJx"
      },
      "source": [
        "## 2.2. Definición del modelo\n",
        "\n",
        "Utilizando la librería Pytorch (https://pytorch.org/docs/stable/index.html), vamos a definir un modelo de ejemplo con una capa LSTM y una capa de salida. La capa de salida estará formada por una única neurona. La salida indicará la probabilidad de voz/silencio utilizando una función *sigmoid*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUBhgYruUKfI"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model_1(nn.Module):\n",
        "    def __init__(self, feat_dim=20):\n",
        "        super(Model_1, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(feat_dim,256,batch_first=True,bidirectional=False)\n",
        "        self.output = nn.Linear(256,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.lstm(x)[0]\n",
        "        out = self.output(out)\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        return out.squeeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-pK445hTOvq"
      },
      "source": [
        "Como se observa, los parámetros indicados serían los siguientes:\n",
        "\n",
        "- Tamaño de la entrada a la capa LSTM: feat_dim\n",
        "- Unidades (celdas) de la capa LSTM: 256\n",
        "- Unidades de salida: 1 (capa lineal)\n",
        "\n",
        "Además, la red LSTM espera un tensor de entrada con el tamaño del batch, la longitud de la secuencia así como la dimensionalidad del espacio de características. La función _forward_ permite obtener la predicción de la salida para un dato (o batch) de entrada (realiza el paso forward del modelo).\n",
        "\n",
        "Revise la documentación de _torch.nn.LSTM_ y preste atención a los argumentos _batch_first_, _bidirectional_ y _dropout_.\n",
        "\n",
        "**PREGUNTA 3: En este modelo, estamos utilizando una única neurona a la salida. ¿Hay alguna otra alternativa? ¿Se seguiría utilizando una función _sigmoid_?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MYcW1sBX8SfG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1M06GDqU31I"
      },
      "source": [
        "Una vez definida la clase, podemos crear nuestra instancia del modelo y cargarlo en la GPU con el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGPBGVSkUacV"
      },
      "source": [
        "model = Model_1(feat_dim=20)\n",
        "model = model.to(torch.device(\"cuda\"))\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2BAp7tVC1A"
      },
      "source": [
        "Nuestra variable _model_ contiene el modelo, y ya estamos listos para entrenarlo y evaluarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZFacm445T6X"
      },
      "source": [
        "##2.3. Lectura y preparación de los datos para el entrenamiento\n",
        "\n",
        "Como hemos visto anteriormente, nuestros datos están guardados en ficheros de Matlab (**.mat**). Cada uno de estos ficheros contiene una matriz **X** correspondiente a las secuencias de características MFCC (con sus derivadas de primer y segundo orden), y un vector **Y** con las etiquetas de voz/silencio correspondientes.\n",
        "\n",
        "Veamos un ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwLhQiBbVnpZ"
      },
      "source": [
        "features_file = 'data/training_VAD/features_labs_1.mat'\n",
        "\n",
        "import scipy.io\n",
        "features = scipy.io.loadmat(features_file)['X']\n",
        "labels = scipy.io.loadmat(features_file)['Y']\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BYGGYYiWNgV"
      },
      "source": [
        "Elija un fichero de entrenamiento y observe tanto el tamaño de **features** como de **labels**. Estas dimensiones se corresponden con la dimensionalidad de las características (20 coeficientes MFCC en nuestro caso) y la otra dimensión es la longitud de la secuencia en número de ventanas o frames."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neZT29yn82H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEhUjoWrXpBj"
      },
      "source": [
        "El entrenamiento del modelo se va a realizar mediante descenso por gradiente (o alguna de sus variantes) basado en _batches_.\n",
        "\n",
        "Para preparar cada uno de estos _batches_ que servirán de entrada a nuestro modelo LSTM, debemos almacenar las características en secuencias de la misma longitud. El siguiente código lee las características (**get_fea**) y sus correspondientes etiquetas (**get_lab**) de un fragmento aleatorio del fichero de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNAp_LJiUjAV"
      },
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "def get_fea(segment, rand_idx):\n",
        "    data = scipy.io.loadmat(segment)['X']\n",
        "    if data.shape[0] <= length_segments:\n",
        "        start_frame = 0\n",
        "    else:\n",
        "        start_frame = np.random.permutation(data.shape[0]-length_segments)[0]\n",
        "\n",
        "    end_frame = np.min((start_frame + length_segments,data.shape[0]))\n",
        "    rand_idx[segment] = start_frame\n",
        "    feat = data[start_frame:end_frame,:]\n",
        "    return feat[np.newaxis, :, :]\n",
        "\n",
        "\n",
        "def get_lab(segment, rand_idx):\n",
        "    data = scipy.io.loadmat(segment)['Y']\n",
        "    start_frame = rand_idx[segment]\n",
        "    end_frame = np.min((start_frame + length_segments, data.shape[0]))\n",
        "    labs = data[start_frame:end_frame].flatten()\n",
        "    return labs[np.newaxis,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wek4EaRoanBq"
      },
      "source": [
        "**PREGUNTA 4: Analice las funciones anteriores detenidamente. ¿De qué tamaño son los fragmentos que se están leyendo? ¿Para qué sirve _rand_idx_?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dtOtqAdM9ETy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrBM4q80bBEK"
      },
      "source": [
        "## 2.4. Entrenamiento del modelo\n",
        "Una vez definidas las funciones de lectura de datos y preparación del formato que necesitamos para la entrada a la red LSTM, podemos utilizar el siguiente código para entrenarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xDupSk1U2li"
      },
      "source": [
        "length_segments = 300\n",
        "path_in_feat = 'data/training_VAD/'\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch_size = 51\n",
        "segment_sets = np.array_split(train_list, len(train_list)/batch_size)\n",
        "\n",
        "max_iters = 5\n",
        "for epoch in range(1, max_iters):\n",
        "  print('Epoch: ',epoch)\n",
        "  model.train()\n",
        "  cache_loss = 0\n",
        "\n",
        "  for ii, segment_set in enumerate(segment_sets):\n",
        "    rand_idx = {}\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Create training batches\n",
        "    train_batch = np.vstack([get_fea(path_in_feat + segment, rand_idx) for segment in segment_set])\n",
        "    labs_batch = np.vstack([get_lab(path_in_feat + segment, rand_idx).astype(np.int16)  for segment in segment_set])\n",
        "    assert len(labs_batch) == len(train_batch) # make sure that all frames have defined label\n",
        "    # Shuffle the data and place them into Pytorch tensors\n",
        "    shuffle = np.random.permutation(len(labs_batch))\n",
        "    labs_batch = torch.tensor(labs_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
        "    train_batch = torch.tensor(train_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
        "\n",
        "    # Forward the data through the network\n",
        "    outputs = model(train_batch)\n",
        "\n",
        "    # Compute cost\n",
        "    loss = criterion(outputs, labs_batch)\n",
        "\n",
        "    # Backward step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    cache_loss += loss.item()\n",
        "\n",
        "  print(\"Loss: \" + str(cache_loss/len(train_batch)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h52xtcubSnD"
      },
      "source": [
        "Analice el código anterior cuidadosamente y preste atención a las siguientes cuestiones:\n",
        "\n",
        "- ¿Qué función de coste se está optimizando?\n",
        "- ¿Qué optimizador se ha definido?\n",
        "- ¿Para qué se utiliza _batch_size_?\n",
        "- ¿Cómo se crean los _batches_?\n",
        "- ¿Qué línea de código realiza el _forward pass_?\n",
        "- ¿Qué línea de código realiza el _backward pass_?\n",
        "\n",
        "**PREGUNTA 5: ¿Cuántas iteraciones del algoritmo ha  realizado? ¿Qué observa en la evolución de la función de coste?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DnX6-F1398pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, añada al código el cálculo de la precisión o _accuracy_, de tal manera que se muestre por pantalla dicho valor en cada iteración (similar a lo que ocurre con el valor del coste _loss_).\n",
        "\n",
        "Observe si el valor de _accuracy_ es alto o bajo, y si hay margen de mejora.\n"
      ],
      "metadata": {
        "id": "JQCKAw1W98-S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FgMFhJ50-L40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_LnYNdQeA91"
      },
      "source": [
        "## 2.5. Evaluación del modelo: un único fichero de test\n",
        "\n",
        "Una vez entrenado el modelo, vamos a evaluarlo en un ejemplo en concreto.\n",
        "\n",
        "Descargue de Moodle el fichero **audio_sample_test.wav**, con sus correspondientes características y etiquetas **audio_sample_test.mat** y evalúe el rendimiento en el mismo, observando por ejemplo el _accuracy_ obtenido.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Código de evaluación aquí"
      ],
      "metadata": {
        "id": "R5tHumyH-QSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwSqBU1zej_R"
      },
      "source": [
        "\n",
        "A continuación, represente 10 segundos de dicho audio, así como sus etiquetas de _ground_truth_ y las obtenidas con su modelo. Visualmente, ¿es bueno el modelo?\n",
        "\n",
        "También puede escuchar el audio para evaluarlo cualitativamente."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Representación 10 segundos + etiquetas estimadas"
      ],
      "metadata": {
        "id": "_KLh6Nkn-njb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Evaluación del modelo: conjunto de validación\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "722dWQLvQDB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a evaluar el rendimiento del modelo anterior sobre un conjunto de validación (del que conocemos sus etiquetas).\n",
        "\n",
        "Para este conjunto de datos, descargaremos la lista de identificadores **valid_VAD.lst** de Moodle, así como el fichero de descarga de datos **data_download_onedrive_valid_VAD.sh**:"
      ],
      "metadata": {
        "id": "zBKhlbtOR1Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!chmod 755 data_download_onedrive_valid_VAD.sh\n",
        "!./data_download_onedrive_valid_VAD.sh"
      ],
      "metadata": {
        "id": "UJs0XS6rSIOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escriba ahora el código necesario para evaluar el modelo anterior en el conjunto de datos de validación, para su última época.\n",
        "\n",
        "Tenga en cuenta que si quiere realizar el forward para todos los datos de validación de una vez, necesitará que todas las secuencias sean de la misma longitud. Como aproximación, puede escoger unos pocos segundos de cada fichero como se hace en el entrenamiento."
      ],
      "metadata": {
        "id": "_lDpCsrORqqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ"
      ],
      "metadata": {
        "id": "AmiIbHbKRvON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Comparación de modelos\n"
      ],
      "metadata": {
        "id": "nsVxachwSa0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.1. Redes LSTM bidireccionales\n",
        "\n",
        "En este apartado, vamos a partir del modelo inicial (_Model_1_) y modificarlo para que la capa LSTM sea bidireccional (_Model_1B_).\n",
        "\n",
        "Entrene el nuevo modelo y compare el resultado con el modelo inicial."
      ],
      "metadata": {
        "id": "XEUWfJmKULm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ"
      ],
      "metadata": {
        "id": "b4gAYAiUTGsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Modelo \"más profundo\"\n",
        "\n",
        "En este apartado, vamos a partir nuevamente del modelo _Model_1_ y vamos a añadir una segunda capa LSTM tras la primera, con el mismo tamaño y configuración, definiendo un nuevo modelo _Model_2_.\n",
        "\n",
        "Entrénelo y compare los resultados."
      ],
      "metadata": {
        "id": "WCYUXhQHTTH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERTE SU CÓDIGO AQUÍ"
      ],
      "metadata": {
        "id": "pG-87vDhTeHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 6: ¿Qué modelo obtiene un mejor resultado sobre los datos de validación, _Model_1_, _Model_1B_ o  _Model_2_? ¿Por qué puede ocurrir esto?**"
      ],
      "metadata": {
        "id": "PEnYNMZ1TjOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G9G9ap4v_JMv"
      }
    }
  ]
}